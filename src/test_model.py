# src/test_model.py
"""
æµ‹è¯•è„šæœ¬ï¼šåŠ è½½å·²ç»è®­ç»ƒå¥½çš„DDPGæ¨¡å‹å¹¶è¯„ä¼°å…¶æ€§èƒ½
"""
import datetime
import numpy as np
import torch
import matplotlib.pyplot as plt
from collections import deque
import os
from src.environment import WeatherEnv
from src.agent import DDPGAgent
from src.config import *
import argparse

class ModelTester:
    """æ¨¡å‹æµ‹è¯•å™¨ï¼Œç”¨äºåŠ è½½å’Œè¯„ä¼°è®­ç»ƒå¥½çš„DDPGæ¨¡å‹"""
    
    def __init__(self, model_path="./models", start_time=datetime.datetime(2020, 1, 8)):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        self.model_path = model_path
        self.device = DEVICE
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
        self.env = WeatherEnv()
        self.agent = DDPGAgent()
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        self.check_model_files()
        
    def check_model_files(self):
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        actor_path = os.path.join(self.model_path, "actor.pth")
        critic_path = os.path.join(self.model_path, "critic.pth")
        
        if not os.path.exists(actor_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°Actoræ¨¡å‹æ–‡ä»¶: {actor_path}")
        if not os.path.exists(critic_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°Criticæ¨¡å‹æ–‡ä»¶: {critic_path}")
            
        print(f"æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶:")
        print(f"  - Actor: {actor_path}")
        print(f"  - Critic: {critic_path}")
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            # åŠ è½½æ¨¡å‹æƒé‡
            self.agent.load(self.model_path)
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            
            # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.agent.actor.eval()
            self.agent.critic.eval()
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise
    
    def evaluate_model(self, num_episodes=10, render=False):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½ - 48å°æ—¶é¢„æµ‹
        
        Args:
            num_episodes: è¯„ä¼°å›åˆæ•°
            render: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
            
        Returns:
            dict: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸
        """
        print(f"\nå¼€å§‹è¯„ä¼°æ¨¡å‹ï¼Œå…±{num_episodes}ä¸ªå›åˆï¼Œé¢„æµ‹æœªæ¥48å°æ—¶...")
        
        episode_rewards = []
        episode_metrics = []
        individual_model_metrics = {name: [] for name in BASE_MODELS}
        ensemble_metrics_history = []
        
        for episode in range(num_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            step_count = 0
            
            if render:
                print(f"\n--- å›åˆ {episode + 1} ---")
            
            for step in range(MAX_STEPS_PER_EPISODE):
                # æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œï¼ˆä¸æ·»åŠ å™ªå£°ï¼‰
                action = self.agent.select_action(state, add_noise=False)
                
                if render:
                    print(f"æ­¥éª¤ {step + 1}:")
                    print(f"  åŠ¨ä½œ (æ¨¡å‹æƒé‡): {dict(zip(BASE_MODELS, action))}")
                
                # ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, info = self.env.step(action)
                
                episode_reward += reward
                step_count += 1
                
                # è®°å½•è¯¦ç»†æŒ‡æ ‡
                if 'ensemble_metrics' in info:
                    ensemble_metrics_history.append(info['ensemble_metrics'])
                    
                    # è®°å½•å„åŸºç¡€æ¨¡å‹æŒ‡æ ‡
                    for model_name in BASE_MODELS:
                        if model_name in info['individual_metrics']:
                            individual_model_metrics[model_name].append(info['individual_metrics'][model_name])
                
                if render:
                    print(f"  å¥–åŠ±: {reward:.4f}")
                    if 'ensemble_metrics' in info:
                        metrics = info['ensemble_metrics']
                        print(f"  é›†æˆRMSE: {metrics['rmse']:.4f}")
                        print(f"  é›†æˆMAE: {metrics['mae']:.4f}")
                        print(f"  é›†æˆç›¸å…³ç³»æ•°: {metrics['correlation']:.4f}")
                        print(f"  é›†æˆMAPE: {metrics['mape']:.2f}%")
                
                state = next_state
                
                if terminated or truncated:
                    break
            
            # è®°å½•å›åˆç»“æœ
            episode_rewards.append(episode_reward)
            
            if render:
                print(f"å›åˆ {episode + 1} å®Œæˆ:")
                print(f"  æ€»å¥–åŠ±: {episode_reward:.4f}")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
        def calculate_average_metrics(metrics_list):
            if not metrics_list:
                return {}
            
            avg_metrics = {}
            for key in metrics_list[0].keys():
                values = [m[key] for m in metrics_list if key in m]
                if values:
                    avg_metrics[f'avg_{key}'] = np.mean(values)
                    avg_metrics[f'std_{key}'] = np.std(values)
                    avg_metrics[f'min_{key}'] = np.min(values)
                    avg_metrics[f'max_{key}'] = np.max(values)
            return avg_metrics
        
        # è®¡ç®—å„åŸºç¡€æ¨¡å‹å¹³å‡æ€§èƒ½
        individual_model_performance = []
        for model_name in BASE_MODELS:
            if individual_model_metrics[model_name]:
                avg_metrics = calculate_average_metrics(individual_model_metrics[model_name])
                individual_model_performance.append({
                    'model': model_name,
                    **avg_metrics
                })
        
        # è®¡ç®—é›†æˆæ¨¡å‹å¹³å‡æ€§èƒ½
        ensemble_performance = calculate_average_metrics(ensemble_metrics_history)
        ensemble_performance['avg_reward'] = np.mean(episode_rewards)
        ensemble_performance['std_reward'] = np.std(episode_rewards)
        
        results = {
            'episode_rewards': episode_rewards,
            'individual_model_performance': individual_model_performance,
            'ensemble_performance': ensemble_performance,
            'individual_model_metrics': individual_model_metrics,
            'ensemble_metrics_history': ensemble_metrics_history
        }
        
        return results
    
    def print_results(self, results):
        """æ‰“å°è¯„ä¼°ç»“æœ - 48å°æ—¶é¢„æµ‹"""
        print("\n" + "="*60)
        print("ğŸ“Š 48å°æ—¶å¤©æ°”é¢„æŠ¥æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("="*60)
        
        ensemble_perf = results['ensemble_performance']
        print(f"\nğŸ¯ DDPGé›†æˆæ¨¡å‹æ€§èƒ½ (48å°æ—¶é¢„æµ‹):")
        print(f"  å¹³å‡å¥–åŠ±: {ensemble_perf['avg_reward']:.4f} Â± {ensemble_perf['std_reward']:.4f}")
        print(f"  å¹³å‡RMSE: {ensemble_perf['avg_rmse']:.4f} Â± {ensemble_perf['std_rmse']:.4f}")
        print(f"  å¹³å‡MAE:  {ensemble_perf['avg_mae']:.4f} Â± {ensemble_perf['std_mae']:.4f}")
        print(f"  å¹³å‡ç›¸å…³ç³»æ•°: {ensemble_perf['avg_correlation']:.4f} Â± {ensemble_perf['std_correlation']:.4f}")
        print(f"  å¹³å‡MAPE: {ensemble_perf['avg_mape']:.2f}% Â± {ensemble_perf['std_mape']:.2f}%")
        print(f"  å¹³å‡NRMSE: {ensemble_perf['avg_nrmse']:.4f} Â± {ensemble_perf['std_nrmse']:.4f}")
        
        print(f"\nğŸ” å„åŸºç¡€æ¨¡å‹æ€§èƒ½ (48å°æ—¶é¢„æµ‹):")
        print(f"{'æ¨¡å‹':<10} {'RMSE':<12} {'MAE':<12} {'ç›¸å…³ç³»æ•°':<10} {'MAPE(%)':<10} {'NRMSE':<10}")
        print("-" * 70)
        
        for model_perf in results['individual_model_performance']:
            print(f"{model_perf['model']:<10} "
                  f"{model_perf['avg_rmse']:.4f}Â±{model_perf['std_rmse']:.4f}  "
                  f"{model_perf['avg_mae']:.4f}Â±{model_perf['std_mae']:.4f}  "
                  f"{model_perf['avg_correlation']:.4f}Â±{model_perf['std_correlation']:.4f}  "
                  f"{model_perf['avg_mape']:.2f}Â±{model_perf['std_mape']:.2f}  "
                  f"{model_perf['avg_nrmse']:.4f}Â±{model_perf['std_nrmse']:.4f}")
        
        # è®¡ç®—æ€§èƒ½æå‡
        individual_rmse = [model['avg_rmse'] for model in results['individual_model_performance']]
        best_individual_rmse = min(individual_rmse)
        rmse_improvement = (best_individual_rmse - ensemble_perf['avg_rmse']) / best_individual_rmse * 100
        
        individual_mae = [model['avg_mae'] for model in results['individual_model_performance']]
        best_individual_mae = min(individual_mae)
        mae_improvement = (best_individual_mae - ensemble_perf['avg_mae']) / best_individual_mae * 100
        
        individual_corr = [model['avg_correlation'] for model in results['individual_model_performance']]
        best_individual_corr = max(individual_corr)
        corr_improvement = (ensemble_perf['avg_correlation'] - best_individual_corr) / best_individual_corr * 100
        
        print(f"\nğŸ“ˆ DDPGé›†æˆæ¨¡å‹æ€§èƒ½æå‡:")
        print(f"  RMSEæå‡: {rmse_improvement:.2f}% (ç›¸æ¯”æœ€ä½³å•ä¸€æ¨¡å‹)")
        print(f"  MAEæå‡:  {mae_improvement:.2f}% (ç›¸æ¯”æœ€ä½³å•ä¸€æ¨¡å‹)")
        print(f"  ç›¸å…³ç³»æ•°æå‡: {corr_improvement:.2f}% (ç›¸æ¯”æœ€ä½³å•ä¸€æ¨¡å‹)")
        
        # æ˜¾ç¤ºé¢„æµ‹æ—¶é—´ä¿¡æ¯
        print(f"\nâ° é¢„æµ‹æ—¶é—´èŒƒå›´: æœªæ¥48å°æ—¶ (æ¯6å°æ—¶ä¸€ä¸ªæ—¶é—´æ­¥)")
        print(f"ğŸ“Š è¯„ä¼°å›åˆæ•°: {len(results['episode_rewards'])}")
    
    def plot_results(self, results, save_path=None):
        """ç»˜åˆ¶è¯„ä¼°ç»“æœå›¾è¡¨ - 48å°æ—¶é¢„æµ‹å¯¹æ¯”"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. å›åˆå¥–åŠ±è¶‹åŠ¿
        ax1.plot(results['episode_rewards'], 'b-', alpha=0.7, label='å›åˆå¥–åŠ±')
        ax1.axhline(y=np.mean(results['episode_rewards']), color='r', linestyle='--', 
                   label=f'å¹³å‡å¥–åŠ±: {np.mean(results["episode_rewards"]):.4f}')
        ax1.set_xlabel('å›åˆ')
        ax1.set_ylabel('å¥–åŠ±')
        ax1.set_title('DDPGé›†æˆæ¨¡å‹ - å›åˆå¥–åŠ±è¶‹åŠ¿ (48å°æ—¶é¢„æµ‹)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. RMSEå¯¹æ¯” - åŸºç¡€æ¨¡å‹ vs DDPGé›†æˆ
        model_names = [model['model'] for model in results['individual_model_performance']]
        model_rmse = [model['avg_rmse'] for model in results['individual_model_performance']]
        
        # æ·»åŠ DDPGé›†æˆæ¨¡å‹
        all_names = model_names + ['DDPGé›†æˆ']
        all_rmse = model_rmse + [results['ensemble_performance']['avg_rmse']]
        colors = ['skyblue', 'lightgreen', 'lightcoral', 'red']
        
        bars = ax2.bar(all_names, all_rmse, alpha=0.7, color=colors)
        ax2.set_ylabel('RMSE')
        ax2.set_title('å„æ¨¡å‹RMSEå¯¹æ¯” (48å°æ—¶é¢„æµ‹)')
        ax2.tick_params(axis='x', rotation=45)
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, rmse in zip(bars, all_rmse):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{rmse:.4f}', ha='center', va='bottom', fontsize=9)
        
        # 3. å¤šæŒ‡æ ‡å¯¹æ¯”é›·è¾¾å›¾
        metrics = ['RMSE', 'MAE', 'ç›¸å…³ç³»æ•°', 'MAPE', 'NRMSE']
        
        # å½’ä¸€åŒ–æŒ‡æ ‡ç”¨äºé›·è¾¾å›¾ (è¶Šå°è¶Šå¥½çš„æŒ‡æ ‡å–å€’æ•°)
        ensemble_metrics = [
            1/results['ensemble_performance']['avg_rmse'],  # RMSE (è¶Šå°è¶Šå¥½)
            1/results['ensemble_performance']['avg_mae'],   # MAE (è¶Šå°è¶Šå¥½)
            results['ensemble_performance']['avg_correlation'],  # ç›¸å…³ç³»æ•° (è¶Šå¤§è¶Šå¥½)
            1/results['ensemble_performance']['avg_mape'],  # MAPE (è¶Šå°è¶Šå¥½)
            1/results['ensemble_performance']['avg_nrmse']  # NRMSE (è¶Šå°è¶Šå¥½)
        ]
        
        # è®¡ç®—æœ€ä½³åŸºç¡€æ¨¡å‹çš„æŒ‡æ ‡
        best_model = min(results['individual_model_performance'], key=lambda x: x['avg_rmse'])
        best_metrics = [
            1/best_model['avg_rmse'],
            1/best_model['avg_mae'],
            best_model['avg_correlation'],
            1/best_model['avg_mape'],
            1/best_model['avg_nrmse']
        ]
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        ensemble_metrics += ensemble_metrics[:1]
        best_metrics += best_metrics[:1]
        
        ax3.plot(angles, ensemble_metrics, 'o-', linewidth=2, label='DDPGé›†æˆ', color='red')
        ax3.fill(angles, ensemble_metrics, alpha=0.25, color='red')
        ax3.plot(angles, best_metrics, 'o-', linewidth=2, label=f'æœ€ä½³åŸºç¡€æ¨¡å‹({best_model["model"]})', color='blue')
        ax3.fill(angles, best_metrics, alpha=0.25, color='blue')
        
        ax3.set_xticks(angles[:-1])
        ax3.set_xticklabels(metrics)
        ax3.set_title('å¤šæŒ‡æ ‡æ€§èƒ½å¯¹æ¯” (å½’ä¸€åŒ–)')
        ax3.legend()
        ax3.grid(True)
        
        # 4. ç›¸å…³ç³»æ•°å¯¹æ¯”
        model_corr = [model['avg_correlation'] for model in results['individual_model_performance']]
        all_corr = model_corr + [results['ensemble_performance']['avg_correlation']]
        
        bars = ax4.bar(all_names, all_corr, alpha=0.7, color=colors)
        ax4.set_ylabel('ç›¸å…³ç³»æ•°')
        ax4.set_title('å„æ¨¡å‹ç›¸å…³ç³»æ•°å¯¹æ¯” (48å°æ—¶é¢„æµ‹)')
        ax4.tick_params(axis='x', rotation=45)
        ax4.grid(True, alpha=0.3)
        ax4.set_ylim(0, 1)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, corr in zip(bars, all_corr):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{corr:.4f}', ha='center', va='bottom', fontsize=9)
        
        plt.suptitle('48å°æ—¶å¤©æ°”é¢„æŠ¥æ¨¡å‹æ€§èƒ½è¯„ä¼°', fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
    
    def run_single_episode_demo(self):
        """è¿è¡Œå•ä¸ªå›åˆæ¼”ç¤ºï¼Œæ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹ - 48å°æ—¶é¢„æµ‹"""
        print("\nğŸ¬ å¼€å§‹å•å›åˆæ¼”ç¤º - 48å°æ—¶å¤©æ°”é¢„æŠ¥...")
        
        state, _ = self.env.reset()
        total_reward = 0
        step_details = []
        
        print(f"åˆå§‹çŠ¶æ€:")
        print(f"  å¤©æ°”æ•°æ®å½¢çŠ¶: {state['weather_data'].shape}")
        print(f"  åˆå§‹æ¨¡å‹RMSE: {dict(zip(BASE_MODELS, state['model_mses']))}")
        print(f"  é¢„æµ‹æ—¶é—´èŒƒå›´: æœªæ¥48å°æ—¶ (æ¯6å°æ—¶ä¸€ä¸ªæ—¶é—´æ­¥)")
        
        for step in range(MAX_STEPS_PER_EPISODE):
            # é€‰æ‹©åŠ¨ä½œ
            action = self.agent.select_action(state, add_noise=False)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, terminated, truncated, info = self.env.step(action)
            
            step_detail = {
                'step': step + 1,
                'action': action,
                'reward': reward,
                'ensemble_metrics': info.get('ensemble_metrics', {}),
                'individual_metrics': info.get('individual_metrics', {})
            }
            step_details.append(step_detail)
            
            total_reward += reward
            
            print(f"\næ­¥éª¤ {step + 1} (é¢„æµ‹æ—¶é—´: {step * 6 + 6} - {step * 6 + 48}å°æ—¶):")
            print(f"  é€‰æ‹©çš„åŠ¨ä½œ (æ¨¡å‹æƒé‡):")
            for i, model_name in enumerate(BASE_MODELS):
                print(f"    {model_name}: {action[i]:.4f}")
            
            print(f"  å¥–åŠ±: {reward:.4f}")
            
            if 'ensemble_metrics' in info:
                metrics = info['ensemble_metrics']
                print(f"  DDPGé›†æˆæ¨¡å‹æ€§èƒ½:")
                print(f"    RMSE: {metrics['rmse']:.4f}")
                print(f"    MAE:  {metrics['mae']:.4f}")
                print(f"    ç›¸å…³ç³»æ•°: {metrics['correlation']:.4f}")
                print(f"    MAPE: {metrics['mape']:.2f}%")
                print(f"    NRMSE: {metrics['nrmse']:.4f}")
            
            print(f"  å„åŸºç¡€æ¨¡å‹æ€§èƒ½:")
            for model_name in BASE_MODELS:
                if model_name in info.get('individual_metrics', {}):
                    model_metrics = info['individual_metrics'][model_name]
                    print(f"    {model_name}:")
                    print(f"      RMSE: {model_metrics['rmse']:.4f}")
                    print(f"      MAE:  {model_metrics['mae']:.4f}")
                    print(f"      ç›¸å…³ç³»æ•°: {model_metrics['correlation']:.4f}")
                    print(f"      MAPE: {model_metrics['mape']:.2f}%")
            
            state = next_state
            
            if terminated or truncated:
                break
        
        print(f"\nğŸ¯ 48å°æ—¶é¢„æµ‹æ¼”ç¤ºå®Œæˆ!")
        print(f"æ€»å¥–åŠ±: {total_reward:.4f}")
        print(f"å¹³å‡å¥–åŠ±: {total_reward/(step+1):.4f}")
        
        # è®¡ç®—æ•´ä½“æ€§èƒ½æå‡
        if step_details and 'ensemble_metrics' in step_details[0]:
            ensemble_rmse = np.mean([step['ensemble_metrics']['rmse'] for step in step_details])
            individual_rmse = {}
            for model_name in BASE_MODELS:
                individual_rmse[model_name] = np.mean([
                    step['individual_metrics'][model_name]['rmse'] 
                    for step in step_details 
                    if model_name in step['individual_metrics']
                ])
            
            best_individual_rmse = min(individual_rmse.values())
            improvement = (best_individual_rmse - ensemble_rmse) / best_individual_rmse * 100
            
            print(f"\nğŸ“ˆ æ€§èƒ½æå‡åˆ†æ:")
            print(f"  æœ€ä½³åŸºç¡€æ¨¡å‹RMSE: {best_individual_rmse:.4f}")
            print(f"  DDPGé›†æˆæ¨¡å‹RMSE: {ensemble_rmse:.4f}")
            print(f"  æ€§èƒ½æå‡: {improvement:.2f}%")
        
        return step_details
    
    def compare_baseline_vs_ddpg(self, num_episodes=5):
        """
        å¯¹æ¯”åŸºç¡€æ¨¡å‹å•ç‹¬é¢„æµ‹ä¸DDPGé›†æˆé¢„æµ‹çš„æ€§èƒ½
        
        Args:
            num_episodes: å¯¹æ¯”å›åˆæ•°
            
        Returns:
            dict: åŒ…å«å¯¹æ¯”ç»“æœçš„å­—å…¸
        """
        print(f"\nğŸ”¬ å¼€å§‹åŸºç¡€æ¨¡å‹ vs DDPGé›†æˆæ¨¡å‹å¯¹æ¯”åˆ†æ...")
        print(f"é¢„æµ‹æ—¶é—´: 48å°æ—¶ | å¯¹æ¯”å›åˆæ•°: {num_episodes}")
        
        baseline_results = {name: [] for name in BASE_MODELS}
        ddpg_results = []
        
        for episode in range(num_episodes):
            print(f"\n--- å¯¹æ¯”å›åˆ {episode + 1} ---")
            
            # é‡ç½®ç¯å¢ƒ
            state, _ = self.env.reset()
            
            for step in range(MAX_STEPS_PER_EPISODE):
                # DDPGé€‰æ‹©åŠ¨ä½œ
                ddpg_action = self.agent.select_action(state, add_noise=False)
                
                # æ‰§è¡ŒDDPGåŠ¨ä½œ
                next_state, ddpg_reward, terminated, truncated, info = self.env.step(ddpg_action)
                
                # è®°å½•DDPGç»“æœ
                if 'ensemble_metrics' in info:
                    ddpg_results.append(info['ensemble_metrics'])
                
                # è®°å½•å„åŸºç¡€æ¨¡å‹ç»“æœ
                for model_name in BASE_MODELS:
                    if model_name in info.get('individual_metrics', {}):
                        baseline_results[model_name].append(info['individual_metrics'][model_name])
                
                state = next_state
                
                if terminated or truncated:
                    break
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        def calculate_avg_metrics(metrics_list):
            if not metrics_list:
                return {}
            avg_metrics = {}
            for key in metrics_list[0].keys():
                values = [m[key] for m in metrics_list if key in m]
                if values:
                    avg_metrics[key] = np.mean(values)
            return avg_metrics
        
        # è®¡ç®—å„æ¨¡å‹å¹³å‡æ€§èƒ½
        baseline_avg = {}
        for model_name in BASE_MODELS:
            baseline_avg[model_name] = calculate_avg_metrics(baseline_results[model_name])
        
        ddpg_avg = calculate_avg_metrics(ddpg_results)
        
        # æ‰“å°å¯¹æ¯”ç»“æœ
        print(f"\n" + "="*80)
        print("ğŸ“Š åŸºç¡€æ¨¡å‹ vs DDPGé›†æˆæ¨¡å‹ - 48å°æ—¶é¢„æµ‹æ€§èƒ½å¯¹æ¯”")
        print("="*80)
        
        print(f"\n{'æ¨¡å‹':<12} {'RMSE':<10} {'MAE':<10} {'ç›¸å…³ç³»æ•°':<10} {'MAPE(%)':<10} {'NRMSE':<10}")
        print("-" * 80)
        
        # æ˜¾ç¤ºå„åŸºç¡€æ¨¡å‹æ€§èƒ½
        for model_name in BASE_MODELS:
            if baseline_avg[model_name]:
                metrics = baseline_avg[model_name]
                print(f"{model_name:<12} "
                      f"{metrics['rmse']:.4f}    "
                      f"{metrics['mae']:.4f}    "
                      f"{metrics['correlation']:.4f}    "
                      f"{metrics['mape']:.2f}    "
                      f"{metrics['nrmse']:.4f}")
        
        # æ˜¾ç¤ºDDPGé›†æˆæ¨¡å‹æ€§èƒ½
        if ddpg_avg:
            print("-" * 80)
            print(f"{'DDPGé›†æˆ':<12} "
                  f"{ddpg_avg['rmse']:.4f}    "
                  f"{ddpg_avg['mae']:.4f}    "
                  f"{ddpg_avg['correlation']:.4f}    "
                  f"{ddpg_avg['mape']:.2f}    "
                  f"{ddpg_avg['nrmse']:.4f}")
        
        # è®¡ç®—æ€§èƒ½æå‡
        if ddpg_avg and baseline_avg:
            print(f"\nğŸ“ˆ DDPGé›†æˆæ¨¡å‹æ€§èƒ½æå‡:")
            
            # æ‰¾åˆ°æœ€ä½³åŸºç¡€æ¨¡å‹
            best_rmse_model = min(baseline_avg.keys(), key=lambda x: baseline_avg[x]['rmse'])
            best_mae_model = min(baseline_avg.keys(), key=lambda x: baseline_avg[x]['mae'])
            best_corr_model = max(baseline_avg.keys(), key=lambda x: baseline_avg[x]['correlation'])
            
            rmse_improvement = (baseline_avg[best_rmse_model]['rmse'] - ddpg_avg['rmse']) / baseline_avg[best_rmse_model]['rmse'] * 100
            mae_improvement = (baseline_avg[best_mae_model]['mae'] - ddpg_avg['mae']) / baseline_avg[best_mae_model]['mae'] * 100
            corr_improvement = (ddpg_avg['correlation'] - baseline_avg[best_corr_model]['correlation']) / baseline_avg[best_corr_model]['correlation'] * 100
            
            print(f"  RMSEæå‡: {rmse_improvement:.2f}% (ç›¸æ¯”æœ€ä½³åŸºç¡€æ¨¡å‹ {best_rmse_model})")
            print(f"  MAEæå‡:  {mae_improvement:.2f}% (ç›¸æ¯”æœ€ä½³åŸºç¡€æ¨¡å‹ {best_mae_model})")
            print(f"  ç›¸å…³ç³»æ•°æå‡: {corr_improvement:.2f}% (ç›¸æ¯”æœ€ä½³åŸºç¡€æ¨¡å‹ {best_corr_model})")
        
        return {
            'baseline_results': baseline_results,
            'ddpg_results': ddpg_results,
            'baseline_avg': baseline_avg,
            'ddpg_avg': ddpg_avg
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æµ‹è¯•è®­ç»ƒå¥½çš„DDPGæ¨¡å‹')
    parser.add_argument('--model_path', type=str, default='./models',
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: ./models)')
    parser.add_argument('--episodes', type=int, default=10,
                       help='è¯„ä¼°å›åˆæ•° (é»˜è®¤: 10)')
    parser.add_argument('--demo', action='store_true',
                       help='è¿è¡Œå•å›åˆè¯¦ç»†æ¼”ç¤º')
    parser.add_argument('--render', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†è¯„ä¼°è¿‡ç¨‹')
    parser.add_argument('--save_plot', type=str, default=None,
                       help='ä¿å­˜å›¾è¡¨çš„è·¯å¾„ (å¯é€‰)')
    parser.add_argument('--compare', action='store_true',
                       help='è¿è¡ŒåŸºç¡€æ¨¡å‹ä¸DDPGé›†æˆæ¨¡å‹çš„å¯¹æ¯”åˆ†æ')
    parser.add_argument('--compare_episodes', type=int, default=1,
                       help='å¯¹æ¯”åˆ†æçš„å›åˆæ•° (é»˜è®¤: 1)')
    parser.add_argument('--datetime', type=datetime.datetime, default=datetime.datetime(2020, 1, 8),
                       help='å¯¹æ¯”åˆ†æçš„èµ·å§‹æ—¶é—´')
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = ModelTester(model_path=args.model_path, start_time=args.datetime)
        
        # åŠ è½½æ¨¡å‹
        tester.load_model()
        
        if args.demo:
            # è¿è¡Œæ¼”ç¤º
            tester.run_single_episode_demo()
        elif args.compare:
            # è¿è¡Œå¯¹æ¯”åˆ†æ
            comparison_results = tester.compare_baseline_vs_ddpg(num_episodes=args.compare_episodes)
        else:
            # è¯„ä¼°æ¨¡å‹
            results = tester.evaluate_model(num_episodes=args.episodes, render=args.render)
            
            # æ‰“å°ç»“æœ
            tester.print_results(results)
            
            # ç»˜åˆ¶å›¾è¡¨
            tester.plot_results(results, save_path=args.save_plot)
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        return 1
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")
    return 0


if __name__ == "__main__":
    exit(main())


